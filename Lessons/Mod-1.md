# Модуль 1 — Введение в нейронные сети и базовая классификация

> Цель модуля: понять, **что такое нейрон и нейронная сеть**, как устроены входной/скрытые/выходной слои, какие бывают **функции активации**, как проходит **прямое распространение (forward)**, какие **типы сетей** встречаются на базовом уровне и чем они отличаются. На практике — собрать простой перцептрон (NumPy) и минимальную MLP (PyTorch) для классификации.

---

## 1. Картина мира: что такое искусственный нейрон и сеть

**Искусственный нейрон** — математический узел, который получает входной вектор x, вычисляет взвешенную сумму и пропускает её через нелинейность:

* Взвешенная сумма: `z = w·x + b`
* Активация: `a = σ(z)`

Где `w` — веса (вектор), `b` — смещение (bias), `σ(·)` — **функция активации**.

**Нейронная сеть** — набор нейронов, соединённых в **слои**. Обычно различают:

* **Входные нейроны:** получают признаки (features) из внешнего мира. Вычислений как таковых не делают, просто передают значения дальше.
* **Скрытые нейроны:** выполняют основную работу (линейное преобразование + нелинейность).
* **Выходные нейроны:** формируют итог — класс, вероятность, число и т.д. Часто здесь стоит специальная активация (например, `softmax`).

> Интуиция: сеть — это последовательность преобразований признаков, где каждый слой извлекает и комбинирует более сложные «фичи», чем предыдущий.

---

## 2. Мини‑класссификация сетей по топологии (базовый уровень)

* **Полносвязные (Fully Connected, FC/MLP):** каждый нейрон слоя `L` связан с каждым нейроном слоя `L+1`.
* **Многослойные (слоистые):** сеть состоит из последовательности слоёв: входной → один/несколько скрытых → выходной.
* **С локальными связями:** связи ограничены ближайшими соседями (пример: карты Кохонена/SOM; в CV — свёрточные сети с локальными фильтрами).

ASCII‑эскиз:

```
Входы → [ Скрытый слой ] → [ Скрытый слой ] → [ Выход ]
   x            h1                 h2             y
```

---

## 3. Модель слоя и обозначения

Обозначим `a^(0) = x` — входной вектор фичей. Тогда для слоя `l = 1..L`:

* Линейная часть: `z^(l) = W^(l) · a^(l-1) + b^(l)`
* Нелинейность: `a^(l) = σ^(l)( z^(l) )`

Где `W^(l)` — матрица весов размера `[n_l × n_{l-1}]`, `b^(l)` — вектор смещений `[n_l]`, `σ^(l)` — активация слоя.

**Выход сети:** `ŷ = a^(L)`.

---

## 4. Функции активации: когда и зачем

| Активация     | Формула/Диапазон                     | Плюсы                                                       | Минусы                            | Где используют                                         |
| ------------- | ------------------------------------ | ----------------------------------------------------------- | --------------------------------- | ------------------------------------------------------ |
| **Sigmoid**   | `σ(z)=1/(1+e^{-z})`, (0;1)           | Интерпретируется как вероятность, гладкая                   | «Затухающие градиенты», насыщение | Выход двоичной классификации (логистическая регрессия) |
| **Tanh**      | `tanh(z)`, (-1;1)                    | Нулевая центрированность, сильнее нелинейность, чем sigmoid | Всё ещё возможно насыщение        | Старые RNN, некоторые скрытые слои                     |
| **ReLU**      | `max(0,z)`                           | Простая, быстро сходится, нет насыщения справа              | «Мёртвые» нейроны при z<0         | Де‑факто стандарт для скрытых слоёв MLP/CNN            |
| **LeakyReLU** | `max(αz, z)`, α≈0.01                 | Реже «умирают» нейроны                                      | Чуть сложнее, чем ReLU            | Альтернатива ReLU                                      |
| **GELU**      | `z·Φ(z)` (≈ `0.5 z (1+tanh(…))`)     | Работает лучше в трансформерах, гладкая                     | Чуть дороже вычислительно         | Трансформеры, современные MLP                          |
| **Softmax**   | `softmax(z)_i = e^{z_i}/∑_j e^{z_j}` | Выдаёт распределение по классам                             | Неприменима в скрытых слоях       | **Выход многоклассовой классификации**                 |

> Правило большого пальца: **скрытые слои** — `ReLU/GELU`, **выход двоичной классификации** — `sigmoid`, **выход многоклассовой** — `softmax`.

---

## 5. Потери (на уровне знакомства)

* **MSE (среднеквадратичная ошибка)**: регрессия.
* **Binary Cross‑Entropy (LogLoss)**: двоичная классификация с `sigmoid`.
* **Categorical Cross‑Entropy**: многоклассовая классификация с `softmax`.

Пока запомним идею: **обучение = минимизация функции потерь** по весам `W, b`.

---

## 6. Forward‑pass пошагово (на примере MLP)

Пусть у нас MLP: `x → Linear(d→h) → ReLU → Linear(h→K) → Softmax`.

1. `z1 = W1 x + b1`
2. `a1 = ReLU(z1)`
3. `z2 = W2 a1 + b2`
4. `ŷ = softmax(z2)`

Дальше считаем **потерю** `L(ŷ, y)` и — в следующих модулях — будем менять веса **в обратном ходе** (backpropagation), чтобы `L` уменьшалась.

---

## 7. Почему нужны нелинейности

Если оставить только линейные преобразования, то композиция линейных операторов остаётся линейной и сеть не сможет аппроксимировать сложные зависимости. Нелинейности (ReLU и др.) дают сети **выразительную мощность**.

---

## 8. Практика A — Перцептрон на NumPy (линейно разделимые классы)

**Задача:** обучить простой перцептрон, разделяющий точки двух классов на плоскости прямой.

```python
import numpy as np

# Генерим игрушечные данные (две «тучки»)
np.random.seed(42)
N = 200
X_pos = np.random.randn(N//2, 2) + np.array([2.0, 2.0])
X_neg = np.random.randn(N//2, 2) + np.array([-2.0, -2.0])
X = np.vstack([X_pos, X_neg])
y = np.hstack([np.ones(N//2), -np.ones(N//2)])  # классы: +1 и -1

# Добавим bias как дополнительный признак = 1
Xb = np.hstack([X, np.ones((N, 1))])

# Инициализация весов
w = np.zeros(Xb.shape[1])

# Перцептрон‑апдейт
lr = 0.1
for epoch in range(50):
    errors = 0
    for i in range(N):
        if y[i] * (Xb[i] @ w) <= 0:  # ошибка классификации
            w += lr * y[i] * Xb[i]
            errors += 1
    if errors == 0:
        break

print("Weights:", w)
# Предсказания
pred = np.sign(Xb @ w)
acc = (pred == y).mean()
print(f"Accuracy: {acc:.3f}")
```

**Что посмотреть:**

* Как влияет `lr` (learning rate) и число эпох.
* Что происходит, если смешать классы сильнее (линейная разделимость нарушается).

---

## 9. Практика B — Мини‑MLP на PyTorch (двухклассовая классификация)

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Данные (нелинейно разделимые)
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_test  = torch.tensor(X_test,  dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
y_test  = torch.tensor(y_test,  dtype=torch.float32).unsqueeze(1)

# Модель: 2 -> 16 -> 1
model = nn.Sequential(
    nn.Linear(2, 16),
    nn.ReLU(),
    nn.Linear(16, 1),
    nn.Sigmoid()
)

criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(2000):
    optimizer.zero_grad()
    y_pred = model(X_train)
    loss = criterion(y_pred, y_train)
    loss.backward()
    optimizer.step()

    if (epoch+1) % 200 == 0:
        with torch.no_grad():
            y_hat = (model(X_test) > 0.5).float()
            acc = (y_hat.eq(y_test)).float().mean().item()
        print(f"Epoch {epoch+1}, loss={loss.item():.4f}, acc={acc:.3f}")
```

**Что посмотреть:**

* Замените `Sigmoid` + `BCELoss` на `nn.Linear(16,2)` + `nn.CrossEntropyLoss()` и `Softmax` не ставьте явно (она внутри `CrossEntropyLoss`).
* Поиграться с размером скрытого слоя (8/16/32) и количеством эпох.

---

## 10. Типичные грабли новичков

* Отсутствие нормализации входов (для MLP это критично).
* Неподходящая активация на выходе (например, `ReLU` вместо `sigmoid/softmax`).
* Слишком большой learning rate → расходимость; слишком маленький → обучение «ползёт».
* Переобучение: нет валидации/регуляризации, тест на тех же данных.

---

## 11. Чек‑лист по итогам модуля

* [ ] Я понимаю разницу между **входными/скрытыми/выходными** нейронами.
* [ ] Могу записать формулы слоя: `z = W a + b`, `a = σ(z)`.
* [ ] Знаю, **какую активацию** ставить на скрытых и выходном слоях.
* [ ] Понимаю, что такое **функция потерь** и зачем она нужна.
* [ ] Могу собрать и обучить простой **перцептрон** и **MLP** для классификации.

---

## 12. Домашнее задание

1. **Перцептрон:** модифицируйте код так, чтобы он работал с классами {0,1} (а не {−1,+1}). Сравните с логистической регрессией (замените правило обновления на градиентный спуск по BCE‑потере).
2. **MLP:** попробуйте 3 слоя (например, 2→32→16→1) и добавьте `Dropout(p=0.2)`. Сравните качество и устойчивость.
3. **Эксперимент с активациями:** замените `ReLU` на `Tanh` и `GELU` — как меняется кривая обучения и финальная точность?

---

## 13. Мини‑FAQ

**Зачем нужен bias (смещение)?**
Даёт модели возможность сдвигать функцию активации, улучшая аппроксимацию даже при нулевых входах.

**Почему softmax только на выходе?**
В скрытых слоях softmax «склеивает» нейроны в распределение, что мешает их независимой выразительности. Поэтому softmax — финальная нормализация вероятностей по классам.

**Нужна ли нормализация признаков?**
Да, почти всегда (StandardScaler/MinMax). Это ускоряет обучение и стабилизирует градиенты.

---

## 14. Что дальше (мостик к Модулю 2–3)

* Модуль 2: сравним архитектуры (полносвязные, локальные/сверточные, рекуррентные), поговорим о параметрах и вычислительной сложности.
* Модуль 3: подробно разберём **backpropagation**, функции потерь и оптимизаторы (SGD, Adam), регуляризацию (L2, dropout, ранняя остановка).

