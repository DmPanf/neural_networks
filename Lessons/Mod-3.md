# –ú–æ–¥—É–ª—å 3 ‚Äî –û–±—É—á–µ–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π

> –¶–µ–ª—å –º–æ–¥—É–ª—è: –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –∏–º–µ–Ω–Ω–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ **—É—á–∞—Ç—Å—è**, —á—Ç–æ —Ç–∞–∫–æ–µ **–æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ (backpropagation)**, –∫–∞–∫–∏–µ –µ—Å—Ç—å **—Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å** –∏ **–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã**, –∫–∞–∫ –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏.

---

## 1. –û–±—â–∞—è –∏–¥–µ—è –æ–±—É—á–µ–Ω–∏—è

–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ = –ø–æ–¥–±–æ—Ä –≤–µ—Å–æ–≤ `W` –∏ —Å–º–µ—â–µ–Ω–∏–π `b`, —á—Ç–æ–±—ã —Å–µ—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–∂–∞–ª–∞ –≤—Ö–æ–¥ `x` –≤ –≤—ã—Ö–æ–¥ `y`.

–§–æ—Ä–º–∞–ª—å–Ω–æ:

* –í—Ö–æ–¥: `x`
* –í—ã—Ö–æ–¥ —Å–µ—Ç–∏: `≈∑(x; W, b)`
* –ü–æ—Ç–µ—Ä–∏: `L(y, ≈∑)`
* –ó–∞–¥–∞—á–∞: –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å `L` –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º `W, b`.

---

## 2. –ü—Ä—è–º–æ–µ –∏ –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ

### üîπ Forward pass

1. –°—á–∏—Ç–∞–µ–º –ª–∏–Ω–µ–π–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏: `z = W¬∑x + b`.
2. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏—é: `a = œÉ(z)`.
3. –ü–æ–≤—Ç–æ—Ä—è–µ–º –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ—ë–≤.
4. –ü–æ–ª—É—á–∞–µ–º `≈∑`.

### üîπ Loss (–æ—à–∏–±–∫–∞)

–°—Ä–∞–≤–Ω–∏–≤–∞–µ–º `≈∑` —Å —Ä–µ–∞–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º `y`.

### üîπ Backpropagation

* –°—á–∏—Ç–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç –æ—à–∏–±–∫–∏ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º: `‚àÇL/‚àÇW`, `‚àÇL/‚àÇb`.
* –ò—Å–ø–æ–ª—å–∑—É–µ–º **–ø—Ä–∞–≤–∏–ª–æ —Ü–µ–ø–æ—á–∫–∏**: –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è—é—Ç—Å—è –æ—Ç –≤—ã—Ö–æ–¥–∞ –∫ –≤—Ö–æ–¥—É.
* –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –ø–æ –ø—Ä–∞–≤–∏–ª—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞:

`W := W - Œ∑ ‚àÇL/‚àÇW`

–ì–¥–µ `Œ∑` ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate).

---

## 3. –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å

| –¢–∏–ø –∑–∞–¥–∞—á–∏                   | –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å                  | –§–æ—Ä–º—É–ª–∞                              |
| ---------------------------- | ------------------------------- | ------------------------------------ |
| –†–µ–≥—Ä–µ—Å—Å–∏—è                    | MSE (—Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞) | `L = (1/n) ‚àë (y - ≈∑)^2`              |
| –î–≤–æ–∏—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è       | Binary Cross-Entropy            | `L = -[ y log(≈∑) + (1-y) log(1-≈∑) ]` |
| –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è | Categorical Cross-Entropy       | `L = - ‚àë y_i log(≈∑_i)`               |

---

## 4. –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã

### üîπ –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (GD)

* –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –Ω–∞ –≤—Å—ë–º –¥–∞—Ç–∞—Å–µ—Ç–µ.
* –ú–∏–Ω—É—Å: –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ.

### üîπ –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (SGD)

* –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –ø—Ä–∏–º–µ—Ä—É.
* –ë—ã—Å—Ç—Ä–æ, –Ω–æ —à—É–º–Ω–æ.

### üîπ –ú–∏–Ω–∏-–±–∞—Ç—á SGD

* –ë–µ—Ä—ë–º –Ω–µ–±–æ–ª—å—à–∏–µ –ø–∞–∫–µ—Ç—ã –¥–∞–Ω–Ω—ã—Ö (batch).
* –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é.

### üîπ –£–ª—É—á—à–µ–Ω–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã

* **Momentum:** —É—Å–∫–æ—Ä—è–µ—Ç –¥–≤–∏–∂–µ–Ω–∏–µ –ø–æ ¬´—É—â–µ–ª—å—è–º¬ª.
* **RMSProp:** –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —à–∞–≥ –ø–æ –∫–∞–∂–¥–æ–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–µ.
* **Adam:** —Å–æ—á–µ—Ç–∞–µ—Ç Momentum + RMSProp. –î–µ—Ñ–æ–ª—Ç–Ω—ã–π –≤—ã–±–æ—Ä.

---

## 5. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è

–ß—Ç–æ–±—ã —Å–µ—Ç—å –Ω–µ –ø–µ—Ä–µ–æ–±—É—á–∞–ª–∞—Å—å, –ø—Ä–∏–º–µ–Ω—è—é—Ç:

* **L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (weight decay):** —à—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ –≤–µ—Å–∞.
* **Dropout:** —Å–ª—É—á–∞–π–Ω–æ ¬´–≤—ã–∫–ª—é—á–∞–µ–º¬ª –Ω–µ–π—Ä–æ–Ω—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.
* **–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ (early stopping):** –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ, –∫–æ–≥–¥–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–µ—Ä–µ—Å—Ç–∞—ë—Ç —É–ª—É—á—à–∞—Ç—å—Å—è.
* **Batch Normalization:** –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ —Å–ª–æ–µ.

---

## 6. –ü—Ä–∞–∫—Ç–∏–∫–∞ A ‚Äî –æ–±—É—á–µ–Ω–∏–µ MLP –Ω–∞ PyTorch

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split

# –î–∞–Ω–Ω—ã–µ
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_test  = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
y_test  = torch.tensor(y_test, dtype=torch.long)

# –ú–æ–¥–µ–ª—å
model = nn.Sequential(
    nn.Linear(2, 16),
    nn.ReLU(),
    nn.Linear(16, 2)
)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

for epoch in range(200):
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    if (epoch+1) % 50 == 0:
        preds = torch.argmax(model(X_test), dim=1)
        acc = (preds == y_test).float().mean().item()
        print(f"Epoch {epoch+1}, loss={loss.item():.4f}, acc={acc:.3f}")
```

---

## 7. –ü—Ä–∞–∫—Ç–∏–∫–∞ B ‚Äî Dropout –∏ Early Stopping

```python
model = nn.Sequential(
    nn.Linear(2, 64),
    nn.ReLU(),
    nn.Dropout(p=0.3),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 2)
)
```

* Dropout –ø–æ–º–æ–≥–∞–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º.
* Early Stopping –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—Ä—É—á–Ω—É—é, –æ—Ç—Å–ª–µ–∂–∏–≤–∞—è `val_loss`.

---

## 8. –ß–µ–∫-–ª–∏—Å—Ç –ø–æ –∏—Ç–æ–≥–∞–º –º–æ–¥—É–ª—è

* [ ] –Ø –ø–æ–Ω–∏–º–∞—é forward –∏ backward pass.
* [ ] –ó–Ω–∞—é, —á—Ç–æ —Ç–∞–∫–æ–µ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –∫–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å BCE/CrossEntropy/MSE.
* [ ] –ú–æ–≥—É –æ–±—ä—è—Å–Ω–∏—Ç—å —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É SGD, Adam, RMSProp.
* [ ] –ó–Ω–∞—é –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏—ë–º—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏.
* [ ] –ú–æ–≥—É –æ–±—É—á–∏—Ç—å –ø—Ä–æ—Å—Ç—É—é MLP –∏ —É–ª—É—á—à–∏—Ç—å –µ—ë —Å –ø–æ–º–æ—â—å—é Dropout.

---

## 9. –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ

1. –°—Ä–∞–≤–Ω–∏—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã SGD –∏ Adam –Ω–∞ –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ –¥–∞—Ç–∞—Å–µ—Ç–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, moons). –ö–∞–∫–æ–π –±—ã—Å—Ç—Ä–µ–µ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ?
2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É: —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏, –∫–æ–≥–¥–∞ `val_loss` –º–∏–Ω–∏–º–∞–ª—å–Ω–æ.
3. –ü–æ—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å Dropout: –ø–æ–ø—Ä–æ–±—É–π—Ç–µ `p=0.1`, `p=0.5`. –ö–∞–∫ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ?

---

## 10. –ú–∏–Ω–∏-FAQ

**–ü–æ—á–µ–º—É Adam –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –≤—ã–±–∏—Ä–∞—é—Ç?**
–ü–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —à–∞–≥–∏ –∏ –±—ã—Å—Ç—Ä–æ —Å—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –∑–∞–¥–∞—á.

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ loss ¬´–∑–∞—Å—Ç—Ä—è–ª¬ª?**
–ü–æ–º–µ–Ω—è—Ç—å lr, –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, –¥–æ–±–∞–≤–∏—Ç—å batch norm.

**–ß—Ç–æ –≤—ã–±—Ä–∞—Ç—å –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏?**
MSE + –ª–∏–Ω–µ–π–Ω—ã–π –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏).

---

## 11. –ß—Ç–æ –¥–∞–ª—å—à–µ

* –ú–æ–¥—É–ª—å 4: —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (Autoencoder, GAN, seq2seq).
* –ú–æ–¥—É–ª—å 5: —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (ResNet, Transformer, BERT, GPT).
