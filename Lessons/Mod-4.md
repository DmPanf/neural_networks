# Модуль 4 — Специализированные архитектуры нейронных сетей

> Цель модуля: познакомиться с архитектурами, которые выходят за рамки базовых MLP/CNN/RNN, понять их идею, применение и реализовать минимальные прототипы.

---

## 1. Автоэнкодеры (Autoencoders)

**Идея:** сеть обучается восстанавливать вход на выходе.

* Состоит из двух частей:

  * **Encoder:** сжимает вход `x` в скрытое представление `z` (latent space).
  * **Decoder:** восстанавливает `x` из `z`.
* Цель: `x ≈ x'` (минимизируем MSE).

Применения:

* Снижение размерности (альтернатива PCA).
* Выделение признаков.
* Удаление шума (Denoising Autoencoder).
* Генерация новых данных (вариационные автоэнкодеры, VAE).

Простейший код:

```python
import torch.nn as nn

class Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(784, 128),
            nn.ReLU(),
            nn.Linear(128, 32))
        self.decoder = nn.Sequential(
            nn.Linear(32, 128),
            nn.ReLU(),
            nn.Linear(128, 784),
            nn.Sigmoid())
    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)
```

---

## 2. Генеративно-состязательные сети (GAN)

**Идея:** состязание двух сетей.

* **Generator (G):** создаёт данные (например, картинки) из случайного шума.
* **Discriminator (D):** пытается отличить реальные данные от сгенерированных.

Процесс обучения:

1. G генерирует данные.
2. D оценивает их как «реальные» или «фейковые».
3. G обновляется так, чтобы обмануть D.
4. D обновляется так, чтобы лучше различать.

Применения:

* Генерация изображений (StyleGAN, BigGAN).
* Суперразрешение.
* Data augmentation.

Простейший код:

```python
class Generator(nn.Module):
    def __init__(self, z_dim=100, out_dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(z_dim, 256),
            nn.ReLU(),
            nn.Linear(256, out_dim),
            nn.Tanh())
    def forward(self, z):
        return self.net(z)

class Discriminator(nn.Module):
    def __init__(self, in_dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid())
    def forward(self, x):
        return self.net(x)
```

---

## 3. Seq2Seq (Sequence-to-Sequence)

**Идея:** преобразование одной последовательности в другую.

* Используется в машинном переводе, чат-ботах, суммаризации.
* Архитектура: **Encoder–Decoder**.

  * Encoder обрабатывает входную последовательность → скрытое состояние.
  * Decoder генерирует выходную последовательность.
* Обычно RNN (LSTM/GRU) или трансформеры.

Простейший вариант (RNN):

```python
class Seq2Seq(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.encoder = nn.RNN(input_dim, hidden_dim, batch_first=True)
        self.decoder = nn.RNN(output_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    def forward(self, src, tgt):
        _, h = self.encoder(src)
        out, _ = self.decoder(tgt, h)
        return self.fc(out)
```

---

## 4. Вариационные автоэнкодеры (VAE)

**Расширение автоэнкодеров:**

* Вместо фиксированного `z` кодируется распределение `q(z|x)`.
* Потери = ошибка восстановления + KL-дивергенция между `q(z|x)` и `p(z)`.
* Даёт возможность **сэмплировать новые данные**.

Применения:

* Генерация изображений.
* Обучение на малых данных.
* Semi-supervised learning.

---

## 5. Сравнительная таблица

| Архитектура | Идея                         | Применение                             |
| ----------- | ---------------------------- | -------------------------------------- |
| Autoencoder | Кодирование и восстановление | Снижение размерности, очистка шума     |
| VAE         | Стохастический автоэнкодер   | Генерация, семплирование               |
| GAN         | Состязание G и D             | Генерация изображений, суперразрешение |
| Seq2Seq     | Encoder–Decoder              | Перевод, чат-боты, суммаризация        |

---

## 6. Практика

* Реализовать автоэнкодер для MNIST, посмотреть латентные представления.
* Написать минимальный GAN и сгенерировать «цифры».
* Сделать Seq2Seq для задачи перевода toy-предложений (например, «cat → кот»).

---

## 7. Чек-лист по итогам модуля

* [ ] Я понимаю разницу между AE, VAE, GAN и Seq2Seq.
* [ ] Могу объяснить идею генератора и дискриминатора.
* [ ] Знаю, что Seq2Seq — это Encoder–Decoder.
* [ ] Могу собрать минимальный AE или GAN на PyTorch.

---

## 8. Домашнее задание

1. Обучите автоэнкодер на MNIST, визуализируйте `z` в 2D (через PCA или t-SNE).
2. Реализуйте GAN и сравните генерацию на разных эпохах.
3. Реализуйте Seq2Seq для toy-примера перевода (алфавит или цифры).

---

## 9. Мини-FAQ

**Почему GANы сложно обучать?**
Потому что нужно сбалансировать обучение G и D: если один слишком силён, другой не учится.

**Чем VAE отличается от GAN?**
VAE учится через вероятностное кодирование, GAN — через состязание.

**Зачем нужны Seq2Seq, если есть трансформеры?**
Трансформеры — это улучшенный Seq2Seq с attention. Но базовый Seq2Seq полезен для понимания.

---

## 10. Что дальше

* Модуль 5: современные модели (ResNet, Transformer, BERT, GPT, Vision Transformer).
* Модуль 6: практика классификации сетей и выбор архитектуры под задачу.
